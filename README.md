
<h1 align="center">ğŸ¤– Olly â€” The Offline AI Data Whisperer</h1>
<p aligh="center">made for oil india as an internship project......</p>

<p align="center">
  <i>Talk to your database like it's ChatGPT â€” but fully offline.</i><br/>
  <strong>Natural Language â†’ SQL â†’ Result</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/status-active-brightgreen?style=flat-square"/>
  <img src="https://img.shields.io/badge/built%20with-Next.js-blue?style=flat-square"/>
  <img src="https://img.shields.io/badge/AI-Ollama%20%2B%20LLaMA3.2-red?style=flat-square"/>
  <img src="https://img.shields.io/badge/db-PostgreSQL-informational?style=flat-square"/>
  <img src="https://img.shields.io/badge/UI-shadcn%2Fui-purple?style=flat-square"/>
</p>

---

## ğŸ§  What is Olly?

**Olly** is a local-first AI chatbot that understands your questions in natural language, converts them to SQL, fetches relevant data from your PostgreSQL database, and gives back meaningful insights â€” all offline.  
No APIs. No servers. No latency.

> ğŸ’¬ _â€œShow me all assets under AMC that expire this monthâ€_  
> Olly â†’ SQL â†’ JSON â†’ Human-readable insights.

---

## ğŸš€ Tech Stack

| Layer       | Technology                          |
|-------------|--------------------------------------|
| UI          | `Next.js`, `Tailwind`, `ShadCN/UI`  |
| Database    | `PostgreSQL`                        |
| AI Engine   | `Ollama` with `LLaMA 3.2`            |
| Container   | `Docker` (multi-service setup)      |
| Optional UI | `Gradio` or custom frontend shell   |

---

## âš™ï¸ Features

- âœ… Offline â€” works without Internet
- ğŸ§  Local LLM inference via LLaMA 3.2
- ğŸ—£ï¸ Natural language â†’ SQL conversion
- ğŸ“Š Renders SQL results into readable format
- ğŸ” Privacy-safe for enterprise/internal tools
- âœ¨ Smooth, modern animated UI (shadcn/ui)

---


## ğŸ–¼ UI Preview

> *Minimal, black-themed, modern interface with animated transitions.*

<p align="center">
  <img src="./photo.jpeg" alt="UI Screenshot" width="800"/>
</p>

> ğŸ¨ Want to preview it? Run locally with Docker or open in Codespaces.

---

## ğŸ§ª How it Works (Internally)

* ğŸ’¬ User Input â†’ LLM prompt (with table context)
* ğŸ“„ Olly (LLaMA3.2 via Ollama) returns SQL query
* ğŸ§µ Executes query using Sequelize/SQLAlchemy
* ğŸ”„ Formats result in a neat, plain answer
* ğŸ–¥ï¸ All running inside Dockerized services

---

## ğŸ›¡ Security

* Runs fully offline on your machine
* No external API calls
* Your schema and data never leave your system

---

## ğŸ“ License

MIT Â© [prax860](https://github.com/prax860)

---

> *â€œOlly doesn't search. Olly *knows*.â€*

